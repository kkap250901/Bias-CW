{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import scipy.stats as stats\n",
    "import researchpy as rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ShortlistedNY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th>ShortlistedNY</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Female</th>\n",
       "      <th>0</th>\n",
       "      <td>0.752475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Male</th>\n",
       "      <th>0</th>\n",
       "      <td>0.512821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.487179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ShortlistedNY\n",
       "Gender ShortlistedNY               \n",
       "Female 0                   0.752475\n",
       "       1                   0.247525\n",
       "Male   0                   0.512821\n",
       "       1                   0.487179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "df_raw = pd.read_excel('recruitment.xls')\n",
    "df = df_raw.replace({\"Gender\": {1: \"Male\", 2: \"Female\"},\n",
    "\"BAMEyn\": {1: \"Black, Asian or Minority Ethnic\", 2: \"No\"},\n",
    "\"FemaleONpanel\": {1: \"Male Only Panel\", 2: \"Female Member On Panel\"}})\n",
    "# \n",
    "# Number of Male Applicants and Number of Female Applicants \n",
    "df_raw.Gender.value_counts()\n",
    "df_raw.BAMEyn.value_counts()\n",
    "# UK discrimnatory Laws to determine gender and \n",
    "pd.DataFrame(df.groupby([\"Gender\"])[\"ShortlistedNY\"].value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Percentages of Offer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>OfferNY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th>OfferNY</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Female</th>\n",
       "      <th>0.0</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Male</th>\n",
       "      <th>0.0</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                OfferNY\n",
       "Gender OfferNY         \n",
       "Female 0.0           18\n",
       "       1.0           10\n",
       "Male   0.0            9\n",
       "       1.0           18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.groupby([\"Gender\"])[\"OfferNY\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Interviewed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th>Interviewed</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Female</th>\n",
       "      <th>0</th>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Male</th>\n",
       "      <th>0</th>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.710526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Interviewed\n",
       "Gender Interviewed             \n",
       "Female 0               0.440000\n",
       "       1               0.560000\n",
       "Male   0               0.289474\n",
       "       1               0.710526"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shortlist = df[df.ShortlistedNY == 1]\n",
    "pd.DataFrame(df_shortlist.groupby([\"Gender\"])[\"Interviewed\"].value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering Gender as a sensitive attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    202\n",
      "1     78\n",
      "Name: Gender, dtype: int64\n",
      "                      ShortlistedNY\n",
      "Gender ShortlistedNY               \n",
      "1      0                   0.512821\n",
      "       1                   0.487179\n",
      "2      0                   0.752475\n",
      "       1                   0.247525\n",
      "                    Interviewed\n",
      "Gender Interviewed             \n",
      "1      0               0.289474\n",
      "       1               0.710526\n",
      "2      0               0.440000\n",
      "       1               0.560000\n",
      "                 OfferNY\n",
      "Gender OfferNY          \n",
      "1      0.0      0.333333\n",
      "       1.0      0.666667\n",
      "2      0.0      0.642857\n",
      "       1.0      0.357143\n",
      "                      FemaleONpanel\n",
      "Gender FemaleONpanel               \n",
      "1      1.0                 0.555556\n",
      "       2.0                 0.444444\n",
      "2      1.0                 0.200000\n",
      "       2.0                 0.800000\n"
     ]
    }
   ],
   "source": [
    "#A lot more females than men\n",
    "print(df_raw.Gender.value_counts())\n",
    "\n",
    "# Gender and Shortlisting \n",
    "print(pd.DataFrame(df_raw.groupby([\"Gender\"])[\"ShortlistedNY\"].value_counts(normalize=True).sort_index()))\n",
    "\n",
    "df_shortlist = df_raw[df_raw.ShortlistedNY == 1]\n",
    "\n",
    "#Shortlisted women getting interviews\n",
    "print(pd.DataFrame(df_shortlist.groupby([\"Gender\"])[\"Interviewed\"].value_counts(normalize=True).sort_index()))\n",
    "\n",
    "#Inteviewed Women getting offers \n",
    "df_interview = df_shortlist[df_shortlist.Interviewed == 1]\n",
    "print(pd.DataFrame(df_interview.groupby([\"Gender\"])[\"OfferNY\"].value_counts(normalize=True).sort_index()))\n",
    "\n",
    "#Interviewed Women Offer ratio based on Female panel\n",
    "df_offer = df_interview[df_interview.OfferNY == 1]\n",
    "print(pd.DataFrame(df_offer.groupby([\"Gender\"])[\"FemaleONpanel\"].value_counts(normalize=True).sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ApplicantCode</th>\n",
       "      <th>Gender</th>\n",
       "      <th>BAMEyn</th>\n",
       "      <th>ShortlistedNY</th>\n",
       "      <th>Interviewed</th>\n",
       "      <th>FemaleONpanel</th>\n",
       "      <th>OfferNY</th>\n",
       "      <th>AcceptNY</th>\n",
       "      <th>JoinYN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>165</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>166</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ApplicantCode  Gender  BAMEyn  ShortlistedNY  Interviewed  FemaleONpanel  \\\n",
       "0                1       1       2              1            1            1.0   \n",
       "1                2       1       2              1            1            1.0   \n",
       "2                3       1       2              1            1            1.0   \n",
       "3                4       1       2              1            1            2.0   \n",
       "19              20       2       2              1            1            2.0   \n",
       "24              25       2       1              1            1            2.0   \n",
       "25              26       2       2              1            1            2.0   \n",
       "140            141       1       2              1            1            1.0   \n",
       "141            142       1       2              1            1            1.0   \n",
       "142            143       1       1              1            1            1.0   \n",
       "143            144       1       1              1            1            2.0   \n",
       "149            150       1       1              1            1            1.0   \n",
       "150            151       1       2              1            1            1.0   \n",
       "151            152       1       2              1            1            1.0   \n",
       "152            153       1       2              1            1            2.0   \n",
       "159            160       2       2              1            1            2.0   \n",
       "164            165       2       2              1            1            2.0   \n",
       "165            166       2       2              1            1            2.0   \n",
       "\n",
       "     OfferNY  AcceptNY  JoinYN  \n",
       "0        1.0       1.0     1.0  \n",
       "1        1.0       1.0     1.0  \n",
       "2        1.0       1.0     1.0  \n",
       "3        1.0       1.0     1.0  \n",
       "19       1.0       1.0     1.0  \n",
       "24       1.0       1.0     1.0  \n",
       "25       1.0       1.0     1.0  \n",
       "140      1.0       1.0     1.0  \n",
       "141      1.0       1.0     1.0  \n",
       "142      1.0       1.0     1.0  \n",
       "143      1.0       1.0     1.0  \n",
       "149      1.0       1.0     1.0  \n",
       "150      1.0       1.0     1.0  \n",
       "151      1.0       1.0     1.0  \n",
       "152      1.0       1.0     1.0  \n",
       "159      1.0       1.0     1.0  \n",
       "164      1.0       1.0     1.0  \n",
       "165      1.0       1.0     1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accept = df_offer[df_offer.AcceptNY == 1]\n",
    "df_accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 AcceptNY\n",
      "Gender AcceptNY          \n",
      "1      0.0       0.333333\n",
      "       1.0       0.666667\n",
      "2      0.0       0.400000\n",
      "       1.0       0.600000\n",
      "               JoinYN\n",
      "Gender JoinYN        \n",
      "1      1.0        1.0\n",
      "2      1.0        1.0\n",
      "               JoinYN\n",
      "Gender JoinYN        \n",
      "1      1.0        1.0\n",
      "2      1.0        1.0\n"
     ]
    }
   ],
   "source": [
    "#Lets see what gender accpets the offer if acceptance rate low then maybe not great benefits for women\n",
    "print(pd.DataFrame(df_offer.groupby([\"Gender\"])[\"AcceptNY\"].value_counts(normalize=True).sort_index()))\n",
    "\n",
    "\n",
    "print(pd.DataFrame(df_accept.groupby([\"Gender\"])[\"JoinYN\"].value_counts(normalize=True).sort_index()))\n",
    "\n",
    "#Lets see out of the people who accpeted if there are any diff in joining \n",
    "df_join = df_accept[df_accept.JoinYN == 1]\n",
    "\n",
    "print(pd.DataFrame(df_accept.groupby([\"Gender\"])[\"JoinYN\"].value_counts(normalize=True).sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab(index = df_raw[\"Gender\"], columns = df[\"OfferNYd\"])\n",
    "df_raw = df_raw.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out indirect Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00019227016669192619\n",
      "Dependent : Reject H0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "croostab = pd.crosstab(df_raw['Gender'],df_raw['ShortlistedNY'])\n",
    "statistic, p, dof, expected = stats.chi2_contingency(croostab)\n",
    "alpha = 0.05 \n",
    "\n",
    "print(p)\n",
    "if p <= alpha:\n",
    "    print('Dependent : Reject H0')\n",
    "else:\n",
    "    print('Independence : Accept H0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4720402657609345e-06\n",
      "Dependent : Reject H0\n"
     ]
    }
   ],
   "source": [
    "croostab = pd.crosstab(df_raw['BAMEyn'],df_raw['ShortlistedNY'])\n",
    "statistic, p, dof, expected = stats.chi2_contingency(croostab)\n",
    "alpha = 0.05 \n",
    "\n",
    "print(p)\n",
    "if p <= alpha:\n",
    "    print('Dependent : Reject H0')\n",
    "else:\n",
    "    print('Independence : Accept H0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that all columns barring the senstive attributes Race and Gender are dependent on sensitive attributes, hence there is indirect prejudice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df_raw.iloc[:,:4]\n",
    "y = df_raw.OfferNY\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, stratify = y)\n",
    "\n",
    "classsfier = LogisticRegression()\n",
    "\n",
    "classsfier.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74  2]\n",
      " [ 6  2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.925     0.974     0.949        76\n",
      "         1.0      0.500     0.250     0.333         8\n",
      "\n",
      "    accuracy                          0.905        84\n",
      "   macro avg      0.713     0.612     0.641        84\n",
      "weighted avg      0.885     0.905     0.890        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = classsfier.predict(x_test)\n",
    "print(metrics.confusion_matrix(y_test,prediction))\n",
    "print(metrics.classification_report(y_test,prediction,digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 0.6365\n",
      "epoch: 20, loss = 0.4860\n",
      "epoch: 30, loss = 0.3969\n",
      "epoch: 40, loss = 0.3395\n",
      "epoch: 50, loss = 0.3000\n",
      "epoch: 60, loss = 0.2714\n",
      "epoch: 70, loss = 0.2498\n",
      "epoch: 80, loss = 0.2330\n",
      "epoch: 90, loss = 0.2196\n",
      "epoch: 100, loss = 0.2087\n",
      "epoch: 110, loss = 0.1997\n",
      "epoch: 120, loss = 0.1921\n",
      "epoch: 130, loss = 0.1856\n",
      "epoch: 140, loss = 0.1800\n",
      "epoch: 150, loss = 0.1752\n",
      "epoch: 160, loss = 0.1709\n",
      "epoch: 170, loss = 0.1671\n",
      "epoch: 180, loss = 0.1638\n",
      "epoch: 190, loss = 0.1608\n",
      "epoch: 200, loss = 0.1580\n",
      "epoch: 210, loss = 0.1556\n",
      "epoch: 220, loss = 0.1533\n",
      "epoch: 230, loss = 0.1513\n",
      "epoch: 240, loss = 0.1494\n",
      "epoch: 250, loss = 0.1477\n",
      "epoch: 260, loss = 0.1461\n",
      "epoch: 270, loss = 0.1446\n",
      "epoch: 280, loss = 0.1433\n",
      "epoch: 290, loss = 0.1420\n",
      "epoch: 300, loss = 0.1408\n",
      "epoch: 310, loss = 0.1397\n",
      "epoch: 320, loss = 0.1386\n",
      "epoch: 330, loss = 0.1376\n",
      "epoch: 340, loss = 0.1367\n",
      "epoch: 350, loss = 0.1358\n",
      "epoch: 360, loss = 0.1350\n",
      "epoch: 370, loss = 0.1342\n",
      "epoch: 380, loss = 0.1335\n",
      "epoch: 390, loss = 0.1328\n",
      "epoch: 400, loss = 0.1321\n",
      "epoch: 410, loss = 0.1315\n",
      "epoch: 420, loss = 0.1309\n",
      "epoch: 430, loss = 0.1303\n",
      "epoch: 440, loss = 0.1298\n",
      "epoch: 450, loss = 0.1293\n",
      "epoch: 460, loss = 0.1288\n",
      "epoch: 470, loss = 0.1283\n",
      "epoch: 480, loss = 0.1278\n",
      "epoch: 490, loss = 0.1274\n",
      "epoch: 500, loss = 0.1269\n",
      "epoch: 510, loss = 0.1265\n",
      "epoch: 520, loss = 0.1261\n",
      "epoch: 530, loss = 0.1258\n",
      "epoch: 540, loss = 0.1254\n",
      "epoch: 550, loss = 0.1251\n",
      "epoch: 560, loss = 0.1247\n",
      "epoch: 570, loss = 0.1244\n",
      "epoch: 580, loss = 0.1241\n",
      "epoch: 590, loss = 0.1238\n",
      "epoch: 600, loss = 0.1235\n",
      "epoch: 610, loss = 0.1232\n",
      "epoch: 620, loss = 0.1229\n",
      "epoch: 630, loss = 0.1227\n",
      "epoch: 640, loss = 0.1224\n",
      "epoch: 650, loss = 0.1222\n",
      "epoch: 660, loss = 0.1219\n",
      "epoch: 670, loss = 0.1217\n",
      "epoch: 680, loss = 0.1215\n",
      "epoch: 690, loss = 0.1212\n",
      "epoch: 700, loss = 0.1210\n",
      "epoch: 710, loss = 0.1208\n",
      "epoch: 720, loss = 0.1206\n",
      "epoch: 730, loss = 0.1204\n",
      "epoch: 740, loss = 0.1202\n",
      "epoch: 750, loss = 0.1200\n",
      "epoch: 760, loss = 0.1199\n",
      "epoch: 770, loss = 0.1197\n",
      "epoch: 780, loss = 0.1195\n",
      "epoch: 790, loss = 0.1194\n",
      "epoch: 800, loss = 0.1192\n",
      "epoch: 810, loss = 0.1190\n",
      "epoch: 820, loss = 0.1189\n",
      "epoch: 830, loss = 0.1187\n",
      "epoch: 840, loss = 0.1186\n",
      "epoch: 850, loss = 0.1184\n",
      "epoch: 860, loss = 0.1183\n",
      "epoch: 870, loss = 0.1182\n",
      "epoch: 880, loss = 0.1180\n",
      "epoch: 890, loss = 0.1179\n",
      "epoch: 900, loss = 0.1178\n",
      "epoch: 910, loss = 0.1176\n",
      "epoch: 920, loss = 0.1175\n",
      "epoch: 930, loss = 0.1174\n",
      "epoch: 940, loss = 0.1173\n",
      "epoch: 950, loss = 0.1172\n",
      "epoch: 960, loss = 0.1171\n",
      "epoch: 970, loss = 0.1169\n",
      "epoch: 980, loss = 0.1168\n",
      "epoch: 990, loss = 0.1167\n",
      "epoch: 1000, loss = 0.1166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9048)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import researchpy as rp\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "df_raw = pd.read_excel('recruitment.xls')\n",
    "df_raw = df_raw.fillna(0)\n",
    "df_raw = df_raw.drop(columns = ['ApplicantCode'])\n",
    "\n",
    "\n",
    "X = df_raw.iloc[:,:4]\n",
    "y = df_raw.OfferNY\n",
    "y = y.values\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
    "X_train_gender = X_train.Gender\n",
    "X_train_gender = X_train_gender.values\n",
    "\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_test1 = X_test\n",
    "y_test1 = y_test\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_trainGender = torch.from_numpy(X_train_gender.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, number_of_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(number_of_features, 1)\n",
    "\n",
    "    def forward(self, x): # Forward for predicting values\n",
    "        Prediction = torch.sigmoid(self.linear(x))\n",
    "        return Prediction\n",
    "\n",
    "    def evaluate(self):\n",
    "        with torch.no_grad():\n",
    "            y_predicted = model(X_test)\n",
    "            y_predicted_cls = y_predicted.round()\n",
    "            acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "        return acc\n",
    "\n",
    "    def train2(self,num_epochs,optimizer,criterion):\n",
    "        for epoch in range(num_epochs):\n",
    "            y_pred = self.forward(X_train) \n",
    "            loss = criterion(y_pred, y_train) # Loss of classifier\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "model = LogisticRegression(n_features) \n",
    "num_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "model.train2(num_epochs,optimizer,criterion) \n",
    "\n",
    "#4) Evaulation\n",
    "model.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversary Debaising Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 0.5012\n",
      "epoch: 20, loss = 0.3996\n",
      "epoch: 30, loss = 0.3377\n",
      "epoch: 40, loss = 0.2966\n",
      "epoch: 50, loss = 0.2676\n",
      "epoch: 60, loss = 0.2461\n",
      "epoch: 70, loss = 0.2295\n",
      "epoch: 80, loss = 0.2164\n",
      "epoch: 90, loss = 0.2058\n",
      "epoch: 100, loss = 0.1970\n",
      "epoch: 110, loss = 0.1896\n",
      "epoch: 120, loss = 0.1834\n",
      "epoch: 130, loss = 0.1780\n",
      "epoch: 140, loss = 0.1733\n",
      "epoch: 150, loss = 0.1692\n",
      "epoch: 160, loss = 0.1656\n",
      "epoch: 170, loss = 0.1623\n",
      "epoch: 180, loss = 0.1594\n",
      "epoch: 190, loss = 0.1568\n",
      "epoch: 200, loss = 0.1544\n",
      "epoch: 210, loss = 0.1523\n",
      "epoch: 220, loss = 0.1503\n",
      "epoch: 230, loss = 0.1485\n",
      "epoch: 240, loss = 0.1468\n",
      "epoch: 250, loss = 0.1453\n",
      "epoch: 260, loss = 0.1438\n",
      "epoch: 270, loss = 0.1425\n",
      "epoch: 280, loss = 0.1413\n",
      "epoch: 290, loss = 0.1401\n",
      "epoch: 300, loss = 0.1391\n",
      "epoch: 310, loss = 0.1380\n",
      "epoch: 320, loss = 0.1371\n",
      "epoch: 330, loss = 0.1362\n",
      "epoch: 340, loss = 0.1353\n",
      "epoch: 350, loss = 0.1345\n",
      "epoch: 360, loss = 0.1338\n",
      "epoch: 370, loss = 0.1331\n",
      "epoch: 380, loss = 0.1324\n",
      "epoch: 390, loss = 0.1317\n",
      "epoch: 400, loss = 0.1311\n",
      "epoch: 410, loss = 0.1305\n",
      "epoch: 420, loss = 0.1300\n",
      "epoch: 430, loss = 0.1294\n",
      "epoch: 440, loss = 0.1289\n",
      "epoch: 450, loss = 0.1284\n",
      "epoch: 460, loss = 0.1280\n",
      "epoch: 470, loss = 0.1275\n",
      "epoch: 480, loss = 0.1271\n",
      "epoch: 490, loss = 0.1267\n",
      "epoch: 500, loss = 0.1263\n",
      "epoch: 510, loss = 0.1259\n",
      "epoch: 520, loss = 0.1255\n",
      "epoch: 530, loss = 0.1252\n",
      "epoch: 540, loss = 0.1248\n",
      "epoch: 550, loss = 0.1245\n",
      "epoch: 560, loss = 0.1242\n",
      "epoch: 570, loss = 0.1239\n",
      "epoch: 580, loss = 0.1236\n",
      "epoch: 590, loss = 0.1233\n",
      "epoch: 600, loss = 0.1230\n",
      "epoch: 610, loss = 0.1228\n",
      "epoch: 620, loss = 0.1225\n",
      "epoch: 630, loss = 0.1222\n",
      "epoch: 640, loss = 0.1220\n",
      "epoch: 650, loss = 0.1218\n",
      "epoch: 660, loss = 0.1215\n",
      "epoch: 670, loss = 0.1213\n",
      "epoch: 680, loss = 0.1211\n",
      "epoch: 690, loss = 0.1209\n",
      "epoch: 700, loss = 0.1207\n",
      "epoch: 710, loss = 0.1205\n",
      "epoch: 720, loss = 0.1203\n",
      "epoch: 730, loss = 0.1201\n",
      "epoch: 740, loss = 0.1199\n",
      "epoch: 750, loss = 0.1197\n",
      "epoch: 760, loss = 0.1196\n",
      "epoch: 770, loss = 0.1194\n",
      "epoch: 780, loss = 0.1192\n",
      "epoch: 790, loss = 0.1191\n",
      "epoch: 800, loss = 0.1189\n",
      "epoch: 810, loss = 0.1188\n",
      "epoch: 820, loss = 0.1186\n",
      "epoch: 830, loss = 0.1185\n",
      "epoch: 840, loss = 0.1183\n",
      "epoch: 850, loss = 0.1182\n",
      "epoch: 860, loss = 0.1181\n",
      "epoch: 870, loss = 0.1179\n",
      "epoch: 880, loss = 0.1178\n",
      "epoch: 890, loss = 0.1177\n",
      "epoch: 900, loss = 0.1176\n",
      "epoch: 910, loss = 0.1174\n",
      "epoch: 920, loss = 0.1173\n",
      "epoch: 930, loss = 0.1172\n",
      "epoch: 940, loss = 0.1171\n",
      "epoch: 950, loss = 0.1170\n",
      "epoch: 960, loss = 0.1169\n",
      "epoch: 970, loss = 0.1168\n",
      "epoch: 980, loss = 0.1167\n",
      "epoch: 990, loss = 0.1166\n",
      "epoch: 1000, loss = 0.1165\n",
      "epoch: 10, loss = 0.1165\n",
      "epoch: 10, loss = 1.9193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/66tsh0jd16n29z6nlxd264n80000gn/T/ipykernel_39773/3853799357.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probas = torch.tensor(model.forward(x), requires_grad= True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss = 0.1165\n",
      "epoch: 20, loss = 1.9193\n",
      "epoch: 30, loss = 0.1165\n",
      "epoch: 30, loss = 1.9193\n",
      "epoch: 40, loss = 0.1165\n",
      "epoch: 40, loss = 1.9193\n",
      "epoch: 50, loss = 0.1165\n",
      "epoch: 50, loss = 1.9193\n",
      "epoch: 60, loss = 0.1165\n",
      "epoch: 60, loss = 1.9193\n",
      "epoch: 70, loss = 0.1165\n",
      "epoch: 70, loss = 1.9193\n",
      "epoch: 80, loss = 0.1165\n",
      "epoch: 80, loss = 1.9193\n",
      "epoch: 90, loss = 0.1165\n",
      "epoch: 90, loss = 1.9193\n",
      "epoch: 100, loss = 0.1165\n",
      "epoch: 100, loss = 1.9193\n",
      "epoch: 110, loss = 0.1165\n",
      "epoch: 110, loss = 1.9193\n",
      "epoch: 120, loss = 0.1165\n",
      "epoch: 120, loss = 1.9193\n",
      "epoch: 130, loss = 0.1165\n",
      "epoch: 130, loss = 1.9193\n",
      "epoch: 140, loss = 0.1165\n",
      "epoch: 140, loss = 1.9193\n",
      "epoch: 150, loss = 0.1165\n",
      "epoch: 150, loss = 1.9193\n",
      "epoch: 160, loss = 0.1165\n",
      "epoch: 160, loss = 1.9193\n",
      "epoch: 170, loss = 0.1165\n",
      "epoch: 170, loss = 1.9193\n",
      "epoch: 180, loss = 0.1165\n",
      "epoch: 180, loss = 1.9193\n",
      "epoch: 190, loss = 0.1165\n",
      "epoch: 190, loss = 1.9193\n",
      "epoch: 200, loss = 0.1165\n",
      "epoch: 200, loss = 1.9193\n",
      "epoch: 210, loss = 0.1165\n",
      "epoch: 210, loss = 1.9193\n",
      "epoch: 220, loss = 0.1165\n",
      "epoch: 220, loss = 1.9193\n",
      "epoch: 230, loss = 0.1165\n",
      "epoch: 230, loss = 1.9193\n",
      "epoch: 240, loss = 0.1165\n",
      "epoch: 240, loss = 1.9193\n",
      "epoch: 250, loss = 0.1165\n",
      "epoch: 250, loss = 1.9193\n",
      "epoch: 260, loss = 0.1165\n",
      "epoch: 260, loss = 1.9193\n",
      "epoch: 270, loss = 0.1165\n",
      "epoch: 270, loss = 1.9193\n",
      "epoch: 280, loss = 0.1165\n",
      "epoch: 280, loss = 1.9193\n",
      "epoch: 290, loss = 0.1165\n",
      "epoch: 290, loss = 1.9193\n",
      "epoch: 300, loss = 0.1165\n",
      "epoch: 300, loss = 1.9193\n",
      "epoch: 310, loss = 0.1165\n",
      "epoch: 310, loss = 1.9193\n",
      "epoch: 320, loss = 0.1165\n",
      "epoch: 320, loss = 1.9193\n",
      "epoch: 330, loss = 0.1165\n",
      "epoch: 330, loss = 1.9193\n",
      "epoch: 340, loss = 0.1165\n",
      "epoch: 340, loss = 1.9193\n",
      "epoch: 350, loss = 0.1165\n",
      "epoch: 350, loss = 1.9193\n",
      "epoch: 360, loss = 0.1165\n",
      "epoch: 360, loss = 1.9193\n",
      "epoch: 370, loss = 0.1165\n",
      "epoch: 370, loss = 1.9193\n",
      "epoch: 380, loss = 0.1165\n",
      "epoch: 380, loss = 1.9193\n",
      "epoch: 390, loss = 0.1165\n",
      "epoch: 390, loss = 1.9193\n",
      "epoch: 400, loss = 0.1165\n",
      "epoch: 400, loss = 1.9193\n",
      "epoch: 410, loss = 0.1165\n",
      "epoch: 410, loss = 1.9193\n",
      "epoch: 420, loss = 0.1165\n",
      "epoch: 420, loss = 1.9193\n",
      "epoch: 430, loss = 0.1165\n",
      "epoch: 430, loss = 1.9193\n",
      "epoch: 440, loss = 0.1165\n",
      "epoch: 440, loss = 1.9193\n",
      "epoch: 450, loss = 0.1165\n",
      "epoch: 450, loss = 1.9193\n",
      "epoch: 460, loss = 0.1165\n",
      "epoch: 460, loss = 1.9193\n",
      "epoch: 470, loss = 0.1165\n",
      "epoch: 470, loss = 1.9193\n",
      "epoch: 480, loss = 0.1165\n",
      "epoch: 480, loss = 1.9193\n",
      "epoch: 490, loss = 0.1165\n",
      "epoch: 490, loss = 1.9193\n",
      "epoch: 500, loss = 0.1165\n",
      "epoch: 500, loss = 1.9193\n",
      "epoch: 510, loss = 0.1165\n",
      "epoch: 510, loss = 1.9193\n",
      "epoch: 520, loss = 0.1165\n",
      "epoch: 520, loss = 1.9193\n",
      "epoch: 530, loss = 0.1165\n",
      "epoch: 530, loss = 1.9193\n",
      "epoch: 540, loss = 0.1165\n",
      "epoch: 540, loss = 1.9193\n",
      "epoch: 550, loss = 0.1165\n",
      "epoch: 550, loss = 1.9193\n",
      "epoch: 560, loss = 0.1165\n",
      "epoch: 560, loss = 1.9193\n",
      "epoch: 570, loss = 0.1165\n",
      "epoch: 570, loss = 1.9193\n",
      "epoch: 580, loss = 0.1165\n",
      "epoch: 580, loss = 1.9193\n",
      "epoch: 590, loss = 0.1165\n",
      "epoch: 590, loss = 1.9193\n",
      "epoch: 600, loss = 0.1165\n",
      "epoch: 600, loss = 1.9193\n",
      "epoch: 610, loss = 0.1165\n",
      "epoch: 610, loss = 1.9193\n",
      "epoch: 620, loss = 0.1165\n",
      "epoch: 620, loss = 1.9193\n",
      "epoch: 630, loss = 0.1165\n",
      "epoch: 630, loss = 1.9193\n",
      "epoch: 640, loss = 0.1165\n",
      "epoch: 640, loss = 1.9193\n",
      "epoch: 650, loss = 0.1165\n",
      "epoch: 650, loss = 1.9193\n",
      "epoch: 660, loss = 0.1165\n",
      "epoch: 660, loss = 1.9193\n",
      "epoch: 670, loss = 0.1165\n",
      "epoch: 670, loss = 1.9193\n",
      "epoch: 680, loss = 0.1165\n",
      "epoch: 680, loss = 1.9193\n",
      "epoch: 690, loss = 0.1165\n",
      "epoch: 690, loss = 1.9193\n",
      "epoch: 700, loss = 0.1165\n",
      "epoch: 700, loss = 1.9193\n",
      "epoch: 710, loss = 0.1165\n",
      "epoch: 710, loss = 1.9193\n",
      "epoch: 720, loss = 0.1165\n",
      "epoch: 720, loss = 1.9193\n",
      "epoch: 730, loss = 0.1165\n",
      "epoch: 730, loss = 1.9193\n",
      "epoch: 740, loss = 0.1165\n",
      "epoch: 740, loss = 1.9193\n",
      "epoch: 750, loss = 0.1165\n",
      "epoch: 750, loss = 1.9193\n",
      "epoch: 760, loss = 0.1165\n",
      "epoch: 760, loss = 1.9193\n",
      "epoch: 770, loss = 0.1165\n",
      "epoch: 770, loss = 1.9193\n",
      "epoch: 780, loss = 0.1165\n",
      "epoch: 780, loss = 1.9193\n",
      "epoch: 790, loss = 0.1165\n",
      "epoch: 790, loss = 1.9193\n",
      "epoch: 800, loss = 0.1165\n",
      "epoch: 800, loss = 1.9193\n",
      "epoch: 810, loss = 0.1165\n",
      "epoch: 810, loss = 1.9193\n",
      "epoch: 820, loss = 0.1165\n",
      "epoch: 820, loss = 1.9193\n",
      "epoch: 830, loss = 0.1165\n",
      "epoch: 830, loss = 1.9193\n",
      "epoch: 840, loss = 0.1165\n",
      "epoch: 840, loss = 1.9193\n",
      "epoch: 850, loss = 0.1165\n",
      "epoch: 850, loss = 1.9193\n",
      "epoch: 860, loss = 0.1165\n",
      "epoch: 860, loss = 1.9193\n",
      "epoch: 870, loss = 0.1165\n",
      "epoch: 870, loss = 1.9193\n",
      "epoch: 880, loss = 0.1165\n",
      "epoch: 880, loss = 1.9193\n",
      "epoch: 890, loss = 0.1165\n",
      "epoch: 890, loss = 1.9193\n",
      "epoch: 900, loss = 0.1165\n",
      "epoch: 900, loss = 1.9193\n",
      "epoch: 910, loss = 0.1165\n",
      "epoch: 910, loss = 1.9193\n",
      "epoch: 920, loss = 0.1165\n",
      "epoch: 920, loss = 1.9193\n",
      "epoch: 930, loss = 0.1165\n",
      "epoch: 930, loss = 1.9193\n",
      "epoch: 940, loss = 0.1165\n",
      "epoch: 940, loss = 1.9193\n",
      "epoch: 950, loss = 0.1165\n",
      "epoch: 950, loss = 1.9193\n",
      "epoch: 960, loss = 0.1165\n",
      "epoch: 960, loss = 1.9193\n",
      "epoch: 970, loss = 0.1165\n",
      "epoch: 970, loss = 1.9193\n",
      "epoch: 980, loss = 0.1165\n",
      "epoch: 980, loss = 1.9193\n",
      "epoch: 990, loss = 0.1165\n",
      "epoch: 990, loss = 1.9193\n",
      "epoch: 1000, loss = 0.1165\n",
      "epoch: 1000, loss = 1.9193\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_raw = pd.read_excel('recruitment.xls')\n",
    "df_raw = df_raw.fillna(0)\n",
    "df_raw = df_raw.drop(columns = ['ApplicantCode'])\n",
    "\n",
    "\n",
    "# 0) Prepare data\n",
    "# bc = datasets.load_breast_cancer()\n",
    "X = df_raw.iloc[:,:4]\n",
    "y = df_raw.OfferNY\n",
    "y = y.values\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
    "X_train_gender = X_train.Gender\n",
    "X_train_gender = X_train_gender.values\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_train = torch.tensor(X_train.astype(np.float32),requires_grad=True)\n",
    "X_trainGender = torch.tensor(X_train_gender.astype(np.float32),requires_grad=True)\n",
    "X_test = torch.tensor(X_test.astype(np.float32),requires_grad=True)\n",
    "y_train = torch.tensor(y_train.astype(np.float32),requires_grad=True)\n",
    "y_test = torch.tensor(y_test.astype(np.float32),requires_grad=True)\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "# Model Logistic Regression\n",
    "\n",
    "X_trainGender = X_trainGender.view(X_train_gender.shape[0], 1)\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, number_of_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(number_of_features, 1)\n",
    "\n",
    "    def forward(self, x): # Forward for predicting values\n",
    "        Prediction = torch.sigmoid(self.linear(x))\n",
    "        return Prediction\n",
    "\n",
    "    def evaluate(self):\n",
    "        with torch.no_grad():\n",
    "            y_predicted = model(X_test)\n",
    "            y_predicted_cls = y_predicted.round()\n",
    "            acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "        return acc\n",
    "\n",
    "    def train2(self,num_epochs,optimizer,criterion):\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            y_pred = self.forward(X_train) \n",
    "            loss = criterion(y_pred, y_train) # Loss of classifier\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "def trainWithAdversary(x, y, sensitive, num_epochs, learning_rate=0.01):\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        #### Compute outputs ####\n",
    "        probas = torch.tensor(model.forward(x), requires_grad= True)\n",
    "        #### Compute labels ####\n",
    "        # labels = torch.where(probas >= .5, 1, 0) # threshold function\n",
    "        # labelsMat = torch.unsqueeze(labels, 1)\n",
    "        # labelsMat = labelsMat.float()\n",
    "        \n",
    "        #### Forward pass adversary ####\n",
    "        adversary_probas = adversary.forward(probas)\n",
    "        \n",
    "        ### Compute loss of Adversary ###\n",
    "        # print(size(sensitive))\n",
    "        lossAdversary = criterion(adversary_probas,sensitive)\n",
    "\n",
    "        ##Compute loss of Regular classifier##\n",
    "        lossClassifier = criterion(probas,y)\n",
    "\n",
    "        # gradientAdv = torch.tensor([1.])\n",
    "        # gradientClass = torch.tensor([1.,1.,1.,1.])\n",
    "        #### Compute gradients ####\n",
    "\n",
    "        classGradient  = x.grad\n",
    "        advGRadient = torch.tensor(1, dtype=torch.float)\n",
    "\n",
    "        # advGRadient = None\n",
    "        lossAdversary.backward(retain_graph=True,gradient = advGRadient) # Gives gradient from labels to sensitive attribute \n",
    "        lossClassifier.backward(retain_graph=True)\n",
    "        sensitive.retain_grad()\n",
    "\n",
    "        nice = []\n",
    "        for (grad,var) in enumerate(classGradient):\n",
    "            unitAdverse = torch.nn.functional.normalize(classGradient[var.long()])\n",
    "            grad -= torch.sum(grad * unitAdverse) * unitAdverse\n",
    "            grad -= lossAdversary * advGRadient.item()\n",
    "            nice.append((grad,var))\n",
    "\n",
    "        classGradient = nice\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # optimizer.step()\n",
    "\n",
    "        # optimizer2.step()\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        # optimizer2.zero_grad()\n",
    "\n",
    "        if (e+1) % 10 == 0:\n",
    "            print(f'epoch: {e+1}, loss = {lossClassifier.item():.4f}')\n",
    "            print(f'epoch: {e+1}, loss = {lossAdversary.item():.4f}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        #### Update classifier weights ####\n",
    "        # print(adversary.weights)\n",
    "        # unit_adversary_grad = [nn.functional.normalize(var[0]) for var in adversary.weights]\n",
    "        # new_grad = []\n",
    "        # model.weights\n",
    "        \n",
    "        # for grad,var in enumerate(classfierWeight):\n",
    "            # A = del W which is classifier weight \n",
    "            # B =  del W which is adversary . weights\n",
    "            # That leaves the projection \n",
    "        # A = classifier_grad_w * cost\n",
    "        # B = classfierWeight_grad * \n",
    "    \n",
    "        # unitAdvesaryVector = torch.norm(ad_grad[grad])\n",
    "        # grad -= torch.sum(grad * unitAdvesaryVector) * unitAdvesaryVector\n",
    "        # grad -= cost * adversary.weights[var.long()]\n",
    "        # new_grad.append((grad,var))\n",
    "        # #Update weights for the classifier\n",
    "        # classifier_grad_w = new_grad\n",
    "        \n",
    "\n",
    "\n",
    "        # if e % 3 == 0:\n",
    "        #     print(labels)\n",
    "        \n",
    "\n",
    "# 1st instance or Model\n",
    "model = LogisticRegression(n_features) \n",
    "adversary = LogisticRegression(1)\n",
    "# 2nd Indtance \n",
    "# mode2 = LogisticRegression(n_features) # Advisory \n",
    "\n",
    "# 2) Loss and optimizer\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.SGD(adversary.parameters(),lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "model.train2(num_epochs,optimizer,criterion)\n",
    "\n",
    "#4) Evaulation\n",
    "model.evaluate()\n",
    "\n",
    "\n",
    "trainWithAdversary(X_train,y_train,X_trainGender,1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('stockprice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f47939dafe77a19b1b206b334b09cf23a00deff041e5c2fa0820ed2fa5af123d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
